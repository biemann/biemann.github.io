---
layout: page
title: Implementation details and architectures
permalink: rlem2021/hyper
author: me
---

Due to the importance of hyperparameters in RL experiments, we aim to provide more details here. Due to the limited page requirements, it was not realistically possible to include it in the original paper. As a disclamer, the architectures between the algorithms differ largely in size, therefore this paper is not meant to be a benchmark study between the algorithms, but more to show that using methods for POMDPs can be valuable for some applications in energy management.

For all experiments, we used a discount factor of $$\gamma=0.99$$. We did not fix a seed in our experiments (something to change in future work). In the paper, we only present the results of one run, but we observed while tuning hyperparameters that each algorithm obtains consistently a similar performence (with the possible exception of SAC-LSTM, which required more tuning to obtain the results of the paper).

## Proximal Policy Optimisation (PPO)

Recall that [PPO](https://arxiv.org/abs/1707.06347) is an actor-critic algorithm. We are interested in finding the policy $$\pi_{\theta}(a\mid o) $$ (or more generally $$\pi_{\theta}(a\mid h)$$). To do this, it is however necessary to predict the rewards in order to update the policy. Therefore, we  also need to train a critic $$V_w(s)$$.

The critic learns the value function $$V_w(s)$$ that minimises the multistep Bellman error:

$$ \frac{1}{2}\mathbb{E}_{\tau \sim p_{\pi}}[(V_w(s_t)-y(s_t,a_t))^2 $$ 

with 

$$y\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1} \gamma^{l} r\left(s_{t+l}, a_{t+l}\right)+\gamma^{T-t} V_{w_{\mathrm{old}}}\left(s_{T}\right).$$

This allows to compute estimates of the advantage function using Generalised Advantage Estimation ([GAE](https://arxiv.org/abs/1506.02438)):

$$ \hat{A}\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1}(\lambda \gamma)^{l}\left(r(s_{t+l},a_{t+l})+\gamma V_w(s_{t+l+1})-V_w(s_{t+l})\right).$$

These estimates are used to update the policy. The policy minimises the following loss:

$$ L_{\pi}(\theta)=\mathbb{E}_{s \sim \rho^{\pi_{\theta_{\text {old }}}}, a \sim \pi_{\theta_{\text {old }}}}\left[\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)} \hat{A}(s, a), g(\varepsilon, \hat{A}(s, a))\right)\right], $$

where

$$ g(\varepsilon, A)=\left\{\begin{array}{ll}
(1+\varepsilon) A & \text { if } A \geq 0, \\
(1-\varepsilon) A & \text { if } A<0.
\end{array}\right. $$

### PPO with feed-forward networks

For PPO with a feed-forward network, we used the implementation from [StableBaselines3](https://github.com/DLR-RM/stable-baselines3). We kept the same hyperparameters and used the same parameters than for our [prior work](https://www.sciencedirect.com/science/article/pii/S0306261921005961). We report the parameters in the following table:

|Critic network | $$5\rightarrow 64 \rightarrow 64 \rightarrow 1 $$ |
|Actor network | $$5 \rightarrow 64 \rightarrow 64 \rightarrow (2\times 4) $$ |
|Activation function | Tanh |
|Optimiser | Adam |
|Learning Rate | $$ 3 \cdot 10^{-4} $$|
|Batch Size | 64 |
|Trace-decay parameter ($$\lambda$$) | 0.95 |
|Horizon (T) | 2048 |
|Number of Epochs | 15 |
|Clipping Range ($$\varepsilon$$) | 0.2 |
|Global Gradient Clipping | 0.5 |
|Entropy regularisation | None |
|Target KL early stopping |None | 

For more information about what these parameters do, we refer to the [documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html). We believe that the results of PPO on this case study can be significantly improved if we take into account the recent [large-scale hyperparameter studies](https://arxiv.org/abs/2006.05990). 

### PPO with LSTM

For an architecture with LSTM, as discussed in the paper, it is more complicated and the two networks are merged into one: 
  ![lstm](/assets/images/lstmppo-1.png){:class="img-responsive"}

As StableBaselines3 does not contain yet an implementation of recurrent policies, we used the implementation of [StableBaselines](https://github.com/hill-a/stable-baselines) instead (in Tensorflow instead of Pytorch). We report the parameters in the following table:

|Neurons of Feed-forward layers (before LSTM cell) | 64 |
|Neurons of LSTM layers | 256 |
|Optimiser | Adam |
|Learning Rate | $$ 3 \cdot 10^{-4} $$|
|Trace-decay parameter ($$\lambda$$) | 0.95 |
|Horizon (T) | 256 |
|Number of Epochs| 4 |
|Clipping range ($$\varepsilon$$) | 0.2 |
|Global Gradient Clipping | 0.5 |
|Layer normalisation | True|
|Entropy regularisation | None |
|Target KL early stopping |None | 

We use a lower horizon to avoid backprogating too far through time. We also use less epochs for updating the networks before collecting new data for better computational efficiency. Note that the choice of architecture can be customised in several different ways. We can also use feed-forward layers after the LSTM cell, use GRU instead of LSTM etc. Other libraries, such as [RLLib](https://github.com/ray-project/ray), use different architectures. 

## Soft Actor Critic (SAC)

[SAC](https://arxiv.org/abs/1812.05905) has shown tremendous success in recent years and became the most popular off-policy RL algorithm for continuous control. The major difference compared to other algorithms comes from the fact that it optimises the maximum entropy RL objective (with $$ \alpha > 0 $$) :

$$ J^{\text{soft}}({\pi}) =\mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t\geq 0}\gamma^t \left(r(s_t, a_t)-\alpha\log\pi(a_t\mid s_t)\right)\right]. $$

Similarily to many off-policy methods, the critic $$Q_w(s,a)$$ is trained to minimise the Bellman error:

$$L(w)=\frac{1}{2}\mathbb{E}_{(s,a,s')\sim \mathcal{U}(D)}\left[\left(y(s,a,s')- Q_w(s,a)\right)^2\right],$$


where

$$ y\left(s, a, s^{\prime}\right)=r(s, a)+\gamma\left(\min _{j=1,2} Q_{\bar{w}_{j}}\left(s^{\prime}, a^{\prime}\right)-\alpha \log \pi_{\theta}\left(a^{\prime} \mid s^{\prime}\right)\right), $$

where $$ a'\sim \pi_{\theta}(\cdot\mid s').$$ We use two target networks to reduce the overestimation bias (as in double Q-learning or TD3).

The policy optimises the following objective in order to satisfy a policy improvement step:

$$ L_{\pi}(\theta)=\mathbb{E}_{s \sim \mathcal{U}(D), a \sim \pi_{\theta}}\left[\alpha \log \pi_{\theta}(a \mid s)-Q_{w}(s, a)\right]. $$

The gradient of this loss strongly resembles the DDPG update (but with additional temperature parameter). It is common to tune the temperature as well, as the performance is sensitive to this hyperparameter and we may want to decrease its value over time (similar to the exploration parameter $$\varepsilon$$ when using $$\varepsilon$$-greedy exploration for Q-learning).

### SAC with feed-forward networks

The following architecture is standard. We used the implementation of [StableBaselines3](https://github.com/DLR-RM/stable-baselines3). The output of the actor is $$(\mu_{\theta}(s), \sigma_{\theta}(s))$$; the action is then sampled from a diagonal multivariate Gaussian distribtion.

 |Number of critics | 2|
 |Critic networks |  $$9\rightarrow 256 \rightarrow 256\rightarrow 1$$ |
 |Actor network | $$5 \rightarrow 256\rightarrow 256 \rightarrow (2\times 4)$$|
 |Activation function | Relu | 
 |Optimiser | Adam |
 |Learning rate | $$3 \cdot 10^{-4}$$|
 |Batch size | 256 |
 |Polyak averaging ($$\tau$$) | $$5 \cdot 10^{-3}$$|
 |Temperature ($$\alpha$$) | Automatically adjusted |

### SAC with LSTM

SAC with LSTM is not really standard in the literature. We found only little implementations, among them [PopularRL algorithms](https://github.com/quantumiracle/Popular-RL-Algorithms), [SeedRL](https://github.com/google-research/seed_rl) and the implementation by [Han et. al](https://github.com/oist-cnru/Variational-Recurrent-Models). We used the first one, as it was a clean implementation. All implementations use the architecture of [Peng et. al](https://arxiv.org/abs/1710.06537). 

|Number of critics | 2|
|Neurons of feed-forward layers of critics | 256 |
|Neurons of LSTM cell of critics | 256 |
|Neurons of feed-forward layers of actor | 128 |
|Neurons of LSTM cell of actor | 256 |
|Sequence length | 20 |
|Batch size | 16 |
|Activation function (outside of LSTM)| Relu |
|Optimiser | Adam |
|Learning rate | $$3 \cdot 10^{-4}$$ |
|Polyak averaging ($$\tau$$) | $$10^{-2}$$ |
| Temperature ($$\alpha$$) | Automatically adjusted | 

The policy network contains two branches: 

1. one recurrent branch has $$(a_{t-1}, o_t)$$ as input, goes through one linear layer, before entering an LSTM cell.
2. the second feed-forward branch has $$o_t $$ as input and goes through one linear layer.
3. the output of both branches are concatenated and go through two other linear layers and has $$(\mu_{\theta}(s), \sigma_{\theta}(s))$$ as output.

The critics also contain two branches:

1. one recurrent branch has $$(a_{t-1}, o_t)$$ as input, goes through one linear layer, before entering an LSTM cell.
2. the second feed-forward branch has $$(a_t, o_t) $$ as input and goes through one linear layer.
3. the output of both branches are concatenated and go through two other linear layers and has its Q-value as output.

The motivation of using two branches is that the recurrent branch is supposed to infer the dynamics of the environment, making the approach arguably model-based. As the current observation $$o_t$$ is of fundamental importance, it is preprocessed by another branch to make sure that this information is still available after going through the LSTM cell.

## Stochastic Latent Actor Critic (SLAC)

[SLAC](https://arxiv.org/abs/1907.00953) is a generalisation of SAC to POMDPs. The derivation of the objective follows the same idea as SAC. It uses the [control as inference](https://arxiv.org/abs/1805.00909) framework, which interprets control as a probabilistic inference problem. This can be used to derive the maximum entropy RL objective that is used by SAC. If we apply the same ideas to a POMDP, instead of an MDP, we arrive at the objective of SLAC.

The actor and critic losses are therefore identical to SAC; the only difference comes from the fact that we also need to approximate the belief. We have, therefore, this loss as well:

$$
\begin{align}
J_M(\varphi, \psi) &= \mathbb{E}_{s_{1:\tau + 1}\sim q_{\varphi}}\Big[\sum_{t=0}^{\tau}\Big(-\log e_{\psi}(o_{t+1}\mid s_{t+1}) \\ &+ \mathcal{D}_{KL}(q_{\varphi}(s_{t+1}\mid o_{t+1}, s_t, a_t)\Vert p_{\psi}(s_{t+1}\mid s_t, a_t))\Big)\Big]. 
\end{align}
$$

We separated the contribution to the encoder weights $$\varphi$$ and the decoder weights $$\psi$$. The intuiton behind this is that the networks have different roles:

1. The encoder $$q_{\varphi}$$ aims to approximate the belief, that is it aims to learn a latent representation of the state. This state can then used by the critic to estimate future rewards.
2. The observation decoder $$e_{\psi}$$ aims to generate samples that resemble the ones given by the environment.
3. The transition decoder $$p_{\psi}$$ aims to model the environment, so that the state is updated when an action is taken in a similar way than the "true" state of the environment. 

Therefore the loss means that on one hand we want to maximise the likelihood of observations $$(\hat{o}_1,\cdots, \hat{o}_{\tau + 1})$$ genrated by the decoder. And on the other hand, we want to minimise the Kullback-Leibler divergence between the sequence of states $$(s_1, \cdots, s_{\tau + 1})$$ inferred by the encoder and the sequence of states $$(\hat{s}_1, \cdots, \hat{s}_{\tau + 1})$$ generated by the decoder. 

Traditional VAE do not evaluate sequences. SLAC uses a Bayesian network to address this issue, but related work also use recurrent VAE. A small (of length 8 in the paper) sequence of observations and actions are collected in the environment (using an older version of the policy). These sequences are then stored into a replay buffer. The networks are then updated the following way:

1. Sample a batch of observation-action sequences $$(o_1, a_1, \cdots, o_{\tau + 1}, a_{\tau + 1})$$ from the replay buffer.
2. The first observation $$o_1$$ goes through the encoder $$q_{\varphi}(s_1\mid o_1)$$. $$q_{\varphi}$$ is a two-layer feed-forward network, that uses $$o_1$$ as input.
3. The latent state $$s_1$$ is used as input by the critic $$Q_w(s_1, a_1)$$.
4. The latent state is used as input by $$e_{\psi}(o_1\mid s_1)$$ to generate an observation $$\hat{o}_1$$.
5. The latent state is also used as input by $$p_{\psi}(s_2\mid s_1, a_1)$$ to generate a next state $$\hat{s}_2$$. This is a network that takes $$(s_1, a_1)$$ as input.
6. The latent state $$s_1$$ is then also used to calculate the next latent state $$s_2$$. Here $$q_{\varphi}(s_2\mid s_1, a_1, o_2)$$ is a neural network that takes $$(s_1,a_1,o_2)$$ as input and is different to the network, used in step 1. 
7. We continue this way to generate the necessary sequences. 
8. We can then calculate the desired losses and update the networks using gradient descent (or Adam in practice).

This implies that we have 7 neural networks to train: 2 critics, 1 actor, 2 decoders and 2 encoders (one for the first time-step using only the initial observation as input, the other using the current observation, along with the previous state and action as input). However, as we shall see, the original implementation is more complicated...

### The actual implementation with latent variable factorisation...

Lee et. al use two-variable factorisation model, meaning that they write $$s_t=(z_t^1, z_t^2)$$ and write 

$$q_{\varphi}(s_1\mid o_1) = q_{\varphi}(z^2_1\mid z_1^1)q_{\varphi}(z_1^1\mid o_1) $$

and

$$q_{\varphi}(s_t\mid o_t, a_{t-1}, s_{t-1})=\textcolor{blue}{q_{\varphi}(z^2_t\mid z^1_t,z^2_{t-1},a_{t-1})}q_{\varphi}(z^1_t\mid o_t, z^2_{t-1},a_{t-1}).$$

We do a similar factorisation for the decoder:

$$p_{\psi}(s_t\mid s_{t-1}, a_{t-1})=\textcolor{blue}{p_{\psi}(z_t^2\mid z^1_t,z^2_{t-1},a_{t-1})}p_{\psi}(z^1_t\mid z^2_{t-1},a_{t-1}).$$

The terms in blue have the same input, so a common simplification is to use the same network $$p_{\psi}(z_t^2\mid z^1_t,z^2_{t-1},a_{t-1})$$ for both the encoder and decoder. This design simplifies the training process. This idea to combine VAE with autoregressive models is for instance discussed by [Maal√∏e et. al.](https://arxiv.org/abs/1902.02102) and has been used in image generation tasks, obtaining competitive performance to generative adversarial networks (GAN). Lee et. al. performed an ablation study and showed that this design significantly improves performance.

Another important design choice is whether the policy $$\pi_{\theta}$$ should use the latent state $$s_t\sim q_{\varphi}$$ as input or be conditioned on the history $$h_t=(o_1,a_1,\cdots, o_t)$$. There is some debate on what choice should be used. SLAC uses the history as input and argues that using the latent state as input leads to over-optimistic behaviour. Furthermore, during runtime, we can run the policy without requiring an inference model. However, [Han et. al.](https://arxiv.org/abs/1912.10703) argues that this choice makes it impossible for the agent to remember long-term dependencies, as the policy only takes into account the recent past. An ablation study is done in the SLAC paper and similar results are obtained. In our experiments, we used the history as imput. However, the critic $$Q_w(s_t,a_t)$$ is conditioned on the state $$s_t\sim q_{\varphi}$$ and obtains significantly worse results otherwise.

Another network is used to model the reward function. However, we believe that for our application, this design choice is not essential, as we defined the reward function with the current observation only, so it is not difficult to infer. However, if the reward function is random or non-Markovian, this additional network may be beneficial. Therefore, using the notations of the two-variable factorisation, we have in addition the reward decoder:

$$\hat{r}_{t+1} \sim r_{\psi}(\cdot\mid z^1_t,z_t^2,a_t,z_{t+1}^1,z_{t+1}^2).$$

Therefore, a sequence of rewards $$(\hat{r}_1, \cdots, \hat{r}_{\tau})$$ is also generated and another likelihood term for the rewards should be added to the loss function. This means that the state should not only include information to explain observations, but also the rewards.

Summing up, the actual loss function for the VAE is the following:

$$
\begin{align} 
J_M(\varphi, \psi) &= \mathbb{E}_{z^1_{1:\tau + 1}\sim q_{\varphi}, z^2_{1:\tau +1}\sim p_{\psi}}\Big[\sum_{t=0}^{\tau}\Big(-\log e_{\psi}(o_{t+1}\mid z^1_{t+1}, z_{t+1}^2) \\ &-\log r_{\psi}(r_{t+1}\mid z^1_{t},z_{t}^2,a_{t},z_{t+1}^1,z_{t+1}^2) \\ &+ \mathcal{D}_{KL}(q_{\varphi}(z^1_{t+1}\mid o_{t+1}, z^2_t, a_t)\Vert p_{\psi}(z^1_{t+1}\mid z^2_t, a_t))\Big)\Big].
\end{align} 
$$

### Implementation details

An important additional change is necessary to adapt the architecture to the current case study. The SLAC paper has been implemented for image observations, so we need to adapt it to vector observations. This for example done by [Han et. al.](https://arxiv.org/abs/1912.10703) and we used the implementation available [here](https://github.com/oist-cnru/Variational-Recurrent-Models). This implementation does the following design choices:

It uses the original version of SAC, which trains two critics: one critic $$Q_w(s,a)$$ and the critic $$V_{\omega}(s)$$. The Bellman targets use $$V_{\bar{\omega}}(s)$$, hence $$V_{\omega}(s)$$ is a network aiming to stabilise the Bellman residuals. The more commonly used version of SAC uses a target Q-network, as it involves training a network less. The implementation however uses temperature tuning, as introduced by SACv2.

They use two different networks to model $$\mu$$ and $$\sigma$$ respectively, each consisting of two layers without shared weights. This is different to the commonly used architectures in RL (e.g. in StableBaselines3), where the input goes through two linear layers and then goes to two separate heads (one for $$\mu$$ and one for $$\sigma$$). In this paper, we used (because we already performed the experiments, before going through the code) the same design than Han et. al., but we believe that a modification here can significantly reduce computational time, without hurting performance.

All neural networks are feed-forward networks and both $$\mu$$ and $$\sigma$$ are trainable parameters. In the original paper, they used a fixed  $$\sigma$$ for $$e_{\psi}$$ to generate images. Furthermore, the actor is conditioned on past observations $$(o_1,\cdots,o_{t})$$ instead of the history $$(o_1,a_1,\cdots, a_{t-1},o_t).$$ The input of the policy is the dimension of observation space * sequence length. To deal with missing data, zero padding is used.

All the networks trained are summarised in the following table:

|Sequence length | 8 |
|Dimension of $$z^1$$ | 16 |
|Dimension of $$z^2$$ | 32 |
|Actor network $$\pi_{\theta}(a_t\mid o_1,\cdots,o_t)$$ | $$2\times(5\cdot 8\rightarrow 256 \rightarrow 256 \rightarrow 4)$$ |
|Value network $$V_{\omega}(z^2_t)$$ | $$32 \rightarrow 256 \rightarrow 256 \rightarrow 1 $$ |
|Q-networks $$Q_{w_i}(z^2_t,a_t)$$ | $$ 2\times ((32+4)\rightarrow 256 \rightarrow 256 \rightarrow 1) $$ |
|Transition decoder $$p_{\psi}(z^1_{t+1}\mid z^2_t, a_t)$$ | $$2\times ((32 + 4)\rightarrow 128 \rightarrow 128 \rightarrow 16)$$|
|Common encoder/decoder $$p_{\psi}(z^2_{t+1}\mid z^1_{t+1},z^2_t,a_t)$$ | $$2\times((32 + 16 + 4)\rightarrow 128 \rightarrow 128 \rightarrow 32)$$|
|Initial transition decoder $$p_{\psi}(z^2_1\mid z^1_1)$$ | $$2\times(16 \rightarrow 128 \rightarrow 128\rightarrow 32)$$|
|Emission decoder $$e_{\psi}(o_t\mid z^1_t,z^2_t)$$ | $$2\times ((32+16)\rightarrow 128\rightarrow 128\rightarrow 5)$$|
|Reward decoder $$r_{\psi}(r_{t+1}\mid z^1_t,z_t^2,a_t,z_{t+1}^1,z_{t+1}^2)$$ | $$2\times((2\cdot(32+16)+4)\rightarrow 128\rightarrow 128\rightarrow 1)$$|
|Belief encoder $$q_{\varphi}(z^1_{t+1}\mid o_{t+1},z^2_t,a_t)$$|$$2\times((32+5+4)\rightarrow 128 \rightarrow 128 \rightarrow 16)$$|
|Initial belief encoder $$q_{\varphi}(z^1_1\mid o_1)$$ | $$2\times (5\rightarrow 128 \rightarrow 128\rightarrow 16)$$|
|Batch size | 64 |
|Activation functions of VAE | Tanh|
|Activation function of actor/critics | Relu |
|Optimiser | Adam|
|Learning rate of VAE | $$10^{-4}$$ |
|Learning rate of actor/critcs | $$3\cdot 10^{-4}$$ |
|Polyak averaging | $$5\cdot 10^{-3}$$|
|Temperature ($$\alpha$$)| Automatically adjusted |
|Model pretraining | 5000 |
|Data collection before training starts | 1000 |

The last two lines mean that we collect data during 1000 timesteps (around 10 days) before pretraining the model. This explains the large performance improvement of SLAC (and SAC-LSTM using a similar trick) at the beginning of the training. This trick can significantly improve data efficiency and may be of interest for other algorithms. 


## Conclusions

The different architectures vary in size by several order of magnitudes, so this is not meant to be a fair comparison between the algorithms. Obviously, SAC-LSTM and SLAC with the current architecture sizes require a GPU. The RAM requirements of SLAC are also high. We found that smaller networks in SLAC (e.g. with around 64 neurons) do not reduce the performence improvements by much, while still enjoying impressive data efficiency. This makes it technically possible to train a model on a CPU, as the additonal gains in data efficiency compensate for the higher computational time.


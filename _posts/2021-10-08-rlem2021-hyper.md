---
layout: page
title: Hyperparameters 
permalink: rlem2021/hyper
author: me
---

Due to the importance of hyperparameters in RL experiments, we aim to provide more details here. Due to the limited page requirements, it was not realistically possible to include it in the original paper. As a disclamer, the architectures between the algorithms differ largely in size, therefore this paper is not meant to be a benchmark study between the algorithms, but more to show that using methods for POMDPs can be valuable for some applications in energy management.

For all experiments, we used a discount factor of $$\gamma=0.99$$. 

## Proximal Policy Optimisation (PPO)

Recall that [PPO](https://arxiv.org/abs/1707.06347) is an actor-critic algorithm. We are interested in finding the policy $$\pi_{\theta}(a\mid o) $$ (or more generally $$\pi_{\theta}(a\mid h)$$). To do this, it is however necessary to predict the rewards in order to update the policy. Therefore, we  also need to train a critic $$V_w(s)$$.

The critic learns the value function $$V_w(s)$$ that minimises the multistep Bellman error:

$$ \frac{1}{2}\mathbb{E}_{\tau \sim p_{\pi}}[(V_w(s_t)-y(s_t,a_t))^2 $$ 

with 

$$y\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1} \gamma^{l} r\left(s_{t+l}, a_{t+l}\right)+\gamma^{T-t} V_{w_{\mathrm{old}}}\left(s_{T}\right).$$

This allows to compute estimates of the advantage function using Generalised Advantage Estimation ([GAE](https://arxiv.org/abs/1506.02438)):

$$ \hat{A}\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1}(\lambda \gamma)^{l}\left(r(s_{t+l},a_{t+l})+\gamma V_w(s_{t+l+1})-V_w(s_{t+l})\right).$$

These estimates are used to update the policy. The policy minimises the following loss:

$$ L_{\pi}(\theta)=\mathbb{E}_{s \sim \rho^{\pi_{\theta_{\text {old }}}}, a \sim \pi_{\theta_{\text {old }}}}\left[\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)} \hat{A}(s, a), g(\varepsilon, \hat{A}(s, a))\right)\right], $$

where

$$ g(\varepsilon, A)=\left\{\begin{array}{ll}
(1+\varepsilon) A & \text { if } A \geq 0, \\
(1-\varepsilon) A & \text { if } A<0.
\end{array}\right. $$

### PPO with feed-forward networks:

For PPO with a feed-forward network, we used the implementation from [StableBaselines3](https://github.com/DLR-RM/stable-baselines3). We kept the same hyperparameters and used the same parameters than for our [prior work](https://www.sciencedirect.com/science/article/pii/S0306261921005961). We report the parameters in the following table:

|Critic network | $$5\rightarrow 64 \rightarrow 64 \rightarrow 1 $$ |
|Actor network | $$5 \rightarrow 64 \rightarrow 64 \rightarrow (2\times 4) $$ |
|Activation function | Tanh |
|Optimiser | Adam |
|Learning Rate | $$ 3 \cdot 10^{-4} $$|
|Batch Size | 64 |
|Trace-decay parameter ($$\lambda$$) | 0.95 |
|Horizon (T) | 2048 |
|Number of Epochs | 15 |
|Clipping Range ($$\varepsilon$$) | 0.2 |
|Global Gradient Clipping | 0.5 |
|Entropy regularisation | None |
|Target KL early stopping |None | 

For more information about what these parameters do, we refer to the [documentation](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html). We believe that the results of PPO on this case study can be significantly improved if we take into account the recent [large-scale hyperparameter studies](https://arxiv.org/abs/2006.05990). 

### PPO with LSTM:

For an architecture with LSTM, as discussed in the paper, it is more complicated and the two networks are merged into one: 
  ![lstm](/assets/images/lstmppo-1.png){:class="img-responsive"}

As StableBaselines3 does not contain yet an implementation of recurrent policies, we used the implementation of [StableBaselines](https://github.com/hill-a/stable-baselines) instead (in Tensorflow instead of Pytorch). We report the parameters in the following table:

|Neurons of Feed-forward layers (before LSTM cell) | 64 |
|Neurons of LSTM layers | 256 |
|Optimiser | Adam |
|Learning Rate | $$ 3 \cdot 10^{-4} $$|
|Trace-decay parameter ($$\lambda$$) | 0.95 |
|Horizon (T) | 256 |
|Number of Epochs| 4 |
|Clipping range ($$\varepsilon$$) | 0.2 |
|Global Gradient Clipping | 0.5 |
|Layer normalisation | True|
|Entropy regularisation | None |
|Target KL early stopping |None | 

We use a lower horizon to avoid backprogating too far through time. We also use less epochs for updating the networks before collecting new data for better computational efficiency. Note that the choice of architecture can be customised in several different ways. We can also use feed-forward layers after the LSTM cell, use GRU instead of LSTM etc. Other libraries, such as [RLLib](https://github.com/ray-project/ray), use different architectures. 

## Soft Actor Critic (SAC)

[SAC](https://arxiv.org/abs/1812.05905) has shown tremendous success in recent years and became the most popular off-policy RL algorithm for continuous control. The major difference compared to other algorithms comes from the fact that it optimises the maximum entropy RL objective (with $$ \alpha > 0 $$) :

$$ J^{\text{soft}}({\pi}) =\mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t\geq 0}\gamma^t \left(r(s_t, a_t)-\alpha\log\pi(a_t\mid s_t)\right)\right]. $$

Similarily to many off-policy methods, the critic $$Q_w(s,a)$$ is trained to minimise the Bellman error:

$$L(w)=\frac{1}{2}\mathbb{E}_{(s,a,s')\sim \mathcal{U}(D)}\left[\left(y(s,a,s')- Q_w(s,a)\right)^2\right],$$


where

$$ y\left(s, a, s^{\prime}\right)=r(s, a)+\gamma\left(\min _{j=1,2} Q_{\bar{w}_{j}}\left(s^{\prime}, a^{\prime}\right)-\alpha \log \pi_{\theta}\left(a^{\prime} \mid s^{\prime}\right)\right), $$

where $$ a'\sim \pi_{\theta}(\cdot\mid s').$$ We use two target networks to reduce the overestimation bias (as in double Q-learning or TD3).

The policy optimises the following objective in order to satisfy a policy improvement step:

$$ L_{\pi}(\theta)=\mathbb{E}_{s \sim \mathcal{U}(D), a \sim \pi_{\theta}}\left[\alpha \log \pi_{\theta}(a \mid s)-Q_{w}(s, a)\right]. $$

The gradient of this loss strongly resembles the DDPG update (but with additional temperature parameter). It is common to tune the temperature as well, as the performance is sensitive to this hyperparameter and we may want to decrease its value over time (similar to the exploration parameter $$\varepsilon$$ when using $$\varepsilon$$-greedy exploration for Q-learning).

### SAC with feed-forward networks:

The following architecture is standard. We used the implementation of [StableBaselines3](https://github.com/DLR-RM/stable-baselines3). The output of the actor is $$(\mu_{\theta}(s), \sigma_{\theta}(s))$$; the action is then sampled from a diagonal multivariate Gaussian distribtion.

 |Number of critics | 2|
 |Critic networks |  $$9\rightarrow 256 \rightarrow 256\rightarrow 1$$ |
 |Actor network | $$5 \rightarrow 256\rightarrow 256 \rightarrow (2\times 4)$$|
 |Activation function | Relu | 
 |Optimiser | Adam |
 |Learning rate | $$3 \cdot 10^{-4}$$|
 |Batch size | 256 |
 |Polyak averaging ($$\tau$$) | $$5 \cdot 10^{-3}$$|
 |Temperature ($$\alpha$$) | Automatically adjusted |

### SAC with LSTM:

SAC with LSTM is not really standard in the literature. We found only little implementations, among them [PopularRL algorithms](https://github.com/quantumiracle/Popular-RL-Algorithms), [SeedRL](https://github.com/google-research/seed_rl) and the implementation by [Han et. al](https://github.com/oist-cnru/Variational-Recurrent-Models). We used the first one, as it was a clean implementation. All implementations use the architecture of [Peng et. al](https://arxiv.org/abs/1710.06537). 

|Number of critics | 2|
|Neurons of feed-forward layers of critics | 256 |
|Neurons of LSTM cell of critics | 256 |
|Neurons of feed-forward layers of actor | 128 |
|Neurons of LSTM cell of actor | 256 |
|Sequence length | 20 |
|Batch size | 16 |
|Activation function (outside of LSTM)| Relu |
|Optimiser | Adam |
|Learning rate | $$3 \cdot 10^{-4}$$ |
|Polyak averaging ($$\tau$$) | $$10^{-2}$$ |
| Temperature ($$\alpha$$) | Automatically adjusted | 

The policy network contains two branches: 

1. one recurrent branch has $$(a_{t-1}, o_t)$$ as input, goes through one linear layer, before entering an LSTM cell.
2. the second feed-forward branch has $$o_t $$ as input and goes through one linear layer.
3. the output of both branches are concatenated and go through two other linear layers and has $$(\mu_{\theta}(s), \sigma_{\theta}(s))$$ as output.

The critics also contain two branches:

1. one recurrent branch has $$(a_{t-1}, o_t)$$ as input, goes through one linear layer, before entering an LSTM cell.
2. the second feed-forward branch has $$(a_t, o_t) $$ as input and goes through one linear layer.
3. the output of both branches are concatenated and go through two other linear layers and has its Q-value as output.

The motivation of using two branches is that the recurrent branch is supposed to infer the dynamics of the environment, making the approach arguably model-based. As the current observation $$o_t$$ is of fundamental importance, it is preprocessed by another branch to make sure that this information is still available after going through the LSTM cell.

## Stochastic Latent Actor Critic (SLAC)

[SLAC](https://arxiv.org/abs/1907.00953) is a generalisation of SAC to POMDPs. The derivation of the objective follows the same idea as SAC. It uses the [control as inference](https://arxiv.org/abs/1805.00909) framework, which interprets control as a probabilistic inference problem. This can be used to derive the maximum entropy RL objective that is used by SAC. If we apply the same ideas to a POMDP, instead of an MDP, we arrive at the objective of SLAC.

The actor and critic losses are therefore identical to SAC; the only difference comes from the fact that we also need to approximate the belief. We have, therefore, this loss as well:

$$J_M(\varphi, \psi) = \mathbb{E}_{s_{1:\tau + 1}\sim q_{\varphi}}\left[\sum_{t=0}^{\tau}\left[-\log e_{\psi}(o_{t+1}\mid s_{t+1}) + \mathcal{D}_{KL}(q_{\varphi}(s_{t+1}\mid o_{t+1}, s_t, a_t)\Vert p_{\psi}(s_{t+1}\mid s_t, a_t))\right]\right]. $$

We separated the contribution to the encoder weights $$\varphi$$ and the decoder weights $$\psi$$. The intuiton behind this is that the networks have different roles:

1. The encoder $$q_{\varphi}$$ aims to approximate the belief, that is it aims to learn a latent representation of the state. This state can then used by the critic to estimate future rewards.
2. The observation decoder $$e_{\psi}$$ aims to generate realistic samples, given by the environment.
3. The transition decoder $$p_{\psi}$$ aims to model the environment, so that the state is updated when an action is taken in a similar way than the "true" state of the environment. 

Therefore the loss means that on one hand we want to maximise the likelihood of observations $$(\hat{o}_1,\cdots, \hat{o}_{\tau + 1})$$ genrated by the decoder. And on the other hand, we want to minimise the Kullback-Leibler divergence between the sequence of states $$(s_1, \cdots, s_{\tau + 1})$$ inferred by the encoder and the sequence of states $$(\hat{s}_1, \cdots, \hat{s}_{\tau + 1})$$ generated by the decoder. 

Traditional VAE do not evaluate sequences. SLAC uses a Bayesian network to address this issue, but related work also use recurrent VAE. A small (of length 8 in the paper) sequence of observations and actions are collected in the environment (using an older version of the policy). These sequences are then stored into a replay buffer. The networks are then updated the following way:

1. Sample a batch of observation-action sequences $$(o_1, a_1, \cdots, o_{\tau + 1}, a_{\tau + 1})$$ from the replay buffer.
2. The first observation $$o_1$$ goes through the encoder $$q_{\varphi}(s_1\mid o_1)$$. $$q_{\varphi}$$ is a two-layer feed-forward network, that uses $$o_1$$ as input.
3. The latent state $$s_1$$ is used as input by the critic $$Q_w(s_1, a_1)$$.
4. The latent state is used as input by $$e_{\psi}(o_1\mid s_1)$$ to generate an observation $$\hat{o}_1$$.
5. The latent state is also used as input by $$p_{\psi}(s_2\mid s_1, a_1)$$ to generate a next state $$\hat{s}_2$$. This is a network that takes $$(s_1, a_1)$$ as input.
6. The latent state $$s_1$$ is then also used to calculate the next latent state $$s_2$$. Here $$q_{\varphi}(s_2\mid s_1, a_1, o_2)$$ is a neural network that takes $$(s_1,a_1,o_2)$$ as input and is different to the network, used in step 1. 
7. We continue this way to generate the necessary sequences. 
8. We can then calculate the desired losses and update the networks using gradient descent (or Adam in practice).

This implies that we have 7 neural networks to train: 2 critics, 1 actor, 2 decoders and 2 encoders (one for the first time-step using only the initial observation as input, the other using the current observation, along with the previous state and action as input). However, as we shall see, the original implementation is more complicated...

### The actual implementation with latent variable factorisation...
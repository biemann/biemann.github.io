I"u<p>Due to the importance of hyperparameters in RL experiments, we aim to provide more details here. Due to the limited page requirements, it was not realistically possible to include it in the original paper. As a disclamer, the architectures between the algorithms differ largely in size, therefore this paper is not meant to be a benchmark study between the algorithms, but more to show that using methods for POMDPs can be valuable for some applications in energy management.</p>

<p>For all experiments, we used a discount factor of \(\gamma=0.99\).</p>

<h2 id="proximal-policy-optimisation-ppo">Proximal Policy Optimisation (PPO)</h2>

<p>Recall that <a href="https://arxiv.org/abs/1707.06347">PPO</a> is an actor-critic algorithm. We are interested in finding the policy \(\pi_{\theta}(a\mid o)\) (or more generally \(\pi_{\theta}(a\mid h)\)). To do this, it is however necessary to predict the rewards in order to update the policy. Therefore, we  also need to train a critic \(V_w(s)\).</p>

<p>The critic learns the value function \(V_w(s)\) that minimises the multistep Bellman error:</p>

\[\frac{1}{2}\mathbb{E}_{\tau \sim p_{\pi}}[(V_w(s_t)-y(s_t,a_t))^2\]

<p>with</p>

\[y\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1} \gamma^{l} r\left(s_{t+l}, a_{t+l}\right)+\gamma^{T-t} V_{w_{\mathrm{old}}}\left(s_{T}\right).\]

<p>This allows to compute estimates of the advantage function using Generalised Advantage Estimation (<a href="https://arxiv.org/abs/1506.02438">GAE</a>):</p>

\[\hat{A}\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1}(\lambda \gamma)^{l}\left(r(s_{t+l},a_{t+l})+\gamma V_w(s_{t+l+1})-V_w(s_{t+l})\right).\]

<p>These estimates are used to update the policy. The policy minimises the following loss:</p>

\[L_{\pi}(\theta)=\mathbb{E}_{s \sim \rho^{\pi_{\theta_{\text {old }}}}, a \sim \pi_{\theta_{\text {old }}}}\left[\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)} \hat{A}(s, a), g(\varepsilon, \hat{A}(s, a))\right)\right],\]

<p>where</p>

\[g(\varepsilon, A)=\left\{\begin{array}{ll}
(1+\varepsilon) A &amp; \text { if } A \geq 0, \\
(1-\varepsilon) A &amp; \text { if } A&lt;0.
\end{array}\right.\]

<h3 id="ppo-with-feed-forward-networks">PPO with feed-forward networks:</h3>

<p>For PPO with a feed-forward network, we used the implementation from <a href="https://github.com/DLR-RM/stable-baselines3">StableBaselines3</a>. We kept the same hyperparameters and used the same parameters than for our <a href="https://www.sciencedirect.com/science/article/pii/S0306261921005961">prior work</a>. We report the parameters in the following table:</p>

<table>
  <tbody>
    <tr>
      <td>Critic network</td>
      <td>\(5\rightarrow 64 \rightarrow 64 \rightarrow 1\)</td>
    </tr>
    <tr>
      <td>Actor network</td>
      <td>\(5 \rightarrow 64 \rightarrow 64 \rightarrow 4\)</td>
    </tr>
    <tr>
      <td>Activation function</td>
      <td>Tanh</td>
    </tr>
    <tr>
      <td>Optimiser</td>
      <td>Adam</td>
    </tr>
    <tr>
      <td>Learning Rate</td>
      <td>\(3 \cdot 10^{-4}\)</td>
    </tr>
    <tr>
      <td>Batch Size</td>
      <td>64</td>
    </tr>
    <tr>
      <td>Trace-decay parameter (\(\lambda\))</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>Horizon (T)</td>
      <td>2048</td>
    </tr>
    <tr>
      <td>Number of Epochs</td>
      <td>15</td>
    </tr>
    <tr>
      <td>Clipping Range (\(\varepsilon\))</td>
      <td>0.2</td>
    </tr>
    <tr>
      <td>Global Gradient Clipping</td>
      <td>0.5</td>
    </tr>
    <tr>
      <td>Entropy regularisation</td>
      <td>None</td>
    </tr>
    <tr>
      <td>Target KL early stopping</td>
      <td>None</td>
    </tr>
  </tbody>
</table>

<p>For more information about what these parameters do, we refer to the <a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">documentation</a>. We believe that the results of PPO on this case study can be significantly improved if we take into account the recent <a href="https://arxiv.org/abs/2006.05990">large-scale hyperparameter studies</a>.</p>

<h3 id="ppo-with-lstm">PPO with LSTM:</h3>

<p>For an architecture with LSTM, as discussed in the paper, it is more complicated and the two networks are merged into one: 
  <img src="/assets/images/lstmppo-1.png" alt="lstm" class="img-responsive" /></p>

<p>As StableBaselines3 does not contain yet an implementation of recurrent policies, we used the implementation of <a href="https://github.com/hill-a/stable-baselines">StableBaselines</a> instead (in Tensorflow instead of Pytorch). We report the parameters in the following table:</p>

<table>
  <tbody>
    <tr>
      <td>Feed-forward layers (before LSTM cell)</td>
      <td>64</td>
    </tr>
    <tr>
      <td>LSTM layers</td>
      <td>256</td>
    </tr>
    <tr>
      <td>Optimiser</td>
      <td>Adam</td>
    </tr>
    <tr>
      <td>Learning Rate</td>
      <td>\(3 \cdot 10^{-4}\)</td>
    </tr>
    <tr>
      <td>Trace-decay parameter (\(\lambda\))</td>
      <td>0.95</td>
    </tr>
    <tr>
      <td>Horizon (T)</td>
      <td>256</td>
    </tr>
    <tr>
      <td>Number of Epochs</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Clipping range (\(\varepsilon\))</td>
      <td>0.2</td>
    </tr>
    <tr>
      <td>Global Gradient Clipping</td>
      <td>0.5</td>
    </tr>
    <tr>
      <td>Layer normalisation</td>
      <td>True</td>
    </tr>
    <tr>
      <td>Entropy regularisation</td>
      <td>None</td>
    </tr>
    <tr>
      <td>Target KL early stopping</td>
      <td>None</td>
    </tr>
  </tbody>
</table>

<p>We use a lower horizon to avoid backprogating too far through time. We also use less epochs for updating the networks before collecting new data for better computational efficiency. Note that the choice of architecture can be customised in several different ways. We can also use feed-forward layers after the LSTM cell, use GRU instead of LSTM etc. Other libraries, such as <a href="https://github.com/ray-project/ray">RLLib</a> use different architectures.</p>

<h2 id="soft-actor-critic-sac">Soft Actor Critic (SAC)</h2>

<p><a href="https://arxiv.org/abs/1812.05905">SAC</a> has shown tremendous success in recent years and became the most popular off-policy RL algorithm for continuous control. The major difference comes from the fact that it optimises the maximum entropy RL objective (with \(\alpha &gt; 0\)) :</p>

\[J^{\text{soft}}({\pi}) =\mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t\geq 0}\gamma^t \left(r(s_t, a_t)-\alpha\log\pi(a_t\mid s_t)\right)\right].\]

<p>Similarily to many off-policy methods, the critic \(Q_w(s,a)\) is trained to minimise the Bellman error:</p>

\[L(w)=\frac{1}{2}\mathbb{E}_{(s,a,s')\sim \mathcal{U}(D)}\left[\left(y(s,a,s')- Q_w(s,a)\right)^2\right],\]

<p>where</p>

\[y\left(s, a, s^{\prime}\right)=r(s, a)+\gamma\left(\min _{j=1,2} Q_{\bar{w}_{j}}\left(s^{\prime}, a^{\prime}\right)-\alpha \log \pi_{\theta}\left(a^{\prime} \mid s^{\prime}\right)\right),\]

<p>where \(a'\sim \pi_{\theta}(\cdot\mid s').\) We use two target networks to reduce the overestimation bias (as in double Q-learning or TD3).</p>

<p>The policy is trained to minimise the Kullback-Leibler divergence between the current policy and the Gibbs (genralised softmax) distribution:</p>
:ET
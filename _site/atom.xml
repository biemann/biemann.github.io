<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Supplementary material</title>
 <link href="/atom.xml" rel="self"/>
 <link href="http://localhost:4000//"/>
 <updated>2021-10-20T17:16:48+02:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Marco Biemann</name>
   <email>marcob@dtu.dk</email>
 </author>

 
 <entry>
   <title>Environment</title>
   <link href="http://localhost:4000//rlem2021/env"/>
   <updated>2021-10-19T00:00:00+02:00</updated>
   <id>http://localhost:4000/rlem2021/rlem2021-env</id>
   <content type="html">&lt;p&gt;This post describes the environment used in the paper more in detail. We performed our experiments on the open source two-zone data centre environment of &lt;a href=&quot;https://github.com/IBM/rl-testbed-for-energyplus&quot;&gt;Moriyama et. al&lt;/a&gt;. We recommend taking a look at &lt;a href=&quot;https://arxiv.org/abs/1808.10427&quot;&gt;their paper&lt;/a&gt;, that describes the environment in far greater detail.&lt;/p&gt;

&lt;p&gt;The HVAC system of the data centre can be described in the following image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hvac_system-1.png&quot; alt=&quot;hvac&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The experiments are done with EnergyPlus v.9.2.0&lt;/p&gt;

&lt;h3 id=&quot;observation-space&quot;&gt;Observation space&lt;/h3&gt;

&lt;p&gt;We used a 5 dimensional observation space:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;Range&lt;/th&gt;
      &lt;th&gt;Unit&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Outdoor air temperature&lt;/td&gt;
      &lt;td&gt;[-20,50]&lt;/td&gt;
      &lt;td&gt;°C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;West zone air temperature&lt;/td&gt;
      &lt;td&gt;[-20,50]&lt;/td&gt;
      &lt;td&gt;°C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;East zone air temperature&lt;/td&gt;
      &lt;td&gt;[-20, 50]&lt;/td&gt;
      &lt;td&gt;°C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IT equipment demand power&lt;/td&gt;
      &lt;td&gt;[0,1]&lt;/td&gt;
      &lt;td&gt;GW&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HVAC equipment demand power&lt;/td&gt;
      &lt;td&gt;[0,1]&lt;/td&gt;
      &lt;td&gt;GW&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;action-space&quot;&gt;Action space&lt;/h3&gt;

&lt;p&gt;We have a 4 dimensional action space:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Variable&lt;/th&gt;
      &lt;th&gt;Range&lt;/th&gt;
      &lt;th&gt;Unit&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;West zone temperature setpoint&lt;/td&gt;
      &lt;td&gt;[10,40]&lt;/td&gt;
      &lt;td&gt;°C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;East zone temperature setpoint&lt;/td&gt;
      &lt;td&gt;[10,40]&lt;/td&gt;
      &lt;td&gt;°C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;West zone fan mass flow rate&lt;/td&gt;
      &lt;td&gt;[1.75, 7.0]&lt;/td&gt;
      &lt;td&gt;kg/s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;East zone fan mass flow rate&lt;/td&gt;
      &lt;td&gt;[1.75, 7.0]&lt;/td&gt;
      &lt;td&gt;kg/s&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;weather-files&quot;&gt;Weather files&lt;/h3&gt;

&lt;p&gt;We used the &lt;a href=&quot;https://energyplus.net/weather&quot;&gt;official EnergyPlus weather&lt;/a&gt; files for our experiments from the following locations:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Years 1-10&lt;/th&gt;
      &lt;th&gt;Years 11-20&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Oslo&lt;/td&gt;
      &lt;td&gt;Hamburg&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bergen&lt;/td&gt;
      &lt;td&gt;Düsseldorf&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Stockholm&lt;/td&gt;
      &lt;td&gt;Bremen&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Karlstad&lt;/td&gt;
      &lt;td&gt;Saint Petersburg&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Kiruna&lt;/td&gt;
      &lt;td&gt;Dundee&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Göteborg&lt;/td&gt;
      &lt;td&gt;Amsterdam&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Tampere&lt;/td&gt;
      &lt;td&gt;Groningen&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Helsinki&lt;/td&gt;
      &lt;td&gt;Gdansk&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Berlin&lt;/td&gt;
      &lt;td&gt;Szcezcin&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The test location uses weather data from Copenhagen. In retroperspective, training for 20 years is very long and not necessary for all algorithms except PPO.&lt;/p&gt;

&lt;h3 id=&quot;modifications-from-the-original-environment&quot;&gt;Modifications from the original environment&lt;/h3&gt;

&lt;p&gt;We performed small modifications to the original environment in order to implement algorithms that are not implemented with OpenAI Baselines. OpenAI Baselines normalises the state and action space automatically, but this is not the case of other implementations that assume that the environment is normalised.&lt;/p&gt;

&lt;p&gt;We modified the following block in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rl-testbed-for-energyplus/gym_energyplus/envs/energyplus_model_2ZoneDataCenterHVAC_wEconomizer_Temp_Fan.py &lt;/code&gt; from&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lo = 10.0
hi = 40.0
flow_hi = 7.0
flow_lo = flow_hi * 0.25
self.action_space = spaces.Box(low=np.array([lo,lo,flow_lo,flow_lo]),
                               high=np.array([hi,hi,flow_hi,flow_hi]),
                               dtype=np.float32)
self.observation_space = spaces.Box(low=np.array(
            [-20.0, -20.0, -20.0, 0.0, 0.0, 0.0]),
            high=np.array(
            [50.0,50.0,50.0,1000000000.0,1000000000.0,1000000000.0]),
            dtype = np.float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.low_action = np.array([10.0, 10.0, 1.75, 1.75])
self.high_action = np.array([40.0, 40.0, 7.0, 7.0])

self.low_obs = np.array([-20.0, -20.0, -20.0, 0.0, 0.0])
self.high_obs = np.array([50.0, 50.0, 50.0, 1e9, 1e9])

self.action_space = spaces.Box(low=-1,high=1,shape=(4,),dtype=np.float32)
self.observation_space = spaces.Box(low=-1,high=1,shape=(5,),dtype=np.float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also need to change the following in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rl-testbed-for-energyplus/gym_energyplus/envs/energyplus_model.py&lt;/code&gt;  in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_action(self, normalized_action)&lt;/code&gt;from&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.action_prev = self.action
self.action = self.action_space.low + 
      (normalized_action + 1.) * 0.5 * 
      (self.action_space.high - self.action_space.low)
self.action = np.clip(
      self.action, self.action_space.low, self.action_space.high)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
self.action_prev = self.action
self.action = np.clip(self.action,
     self.action_space.low, self.action_space.high)
self.action = self.low_action + (normalized_action + 1.) 
    * 0.5 * (self.high_action - self.low_action)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we did the following modifications to the EnergyPlus controller file. With this change, we were able to reproduce the results by Moriyama et. al. in their paper. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2ZoneDataCenterHVAC_wEconomizer-baseline.idf&lt;/code&gt; (l. 2984), we changed the air flow calculation method.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; ElectricEquipment:ITE:AirCooled,
    EastDataCenter_Equip,    !- Name
    East Zone,               !- Zone Name
    !FlowControlWithApproachTemperatures,  !- Air Flow Calculation Method
    FlowFromSystem,            ! New Air Flow Calculation Method
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;modifications-to-the-libraries&quot;&gt;Modifications to the libraries&lt;/h3&gt;

&lt;p&gt;We mostly kept the code implementations intact, except for some hyperparameters we tuned. Some small modifications were nonetheless necessary.&lt;/p&gt;

&lt;p&gt;We modified the SLAC implementation by &lt;a href=&quot;https://github.com/oist-cnru/Variational-Recurrent-Models&quot;&gt;Han et. al.&lt;/a&gt; in the following two ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We avoid resetting the environment after 1000 steps. Resetting the environment means starting a new year. Instead of resetting, we continue the experiment in the same episode.&lt;/li&gt;
  &lt;li&gt;We modified the implementation to make it run on a GPU.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We also did the modification #1 in the implementation of &lt;a href=&quot;https://github.com/quantumiracle/Popular-RL-Algorithms&quot;&gt;SAC-LSTM&lt;/a&gt; from Ding.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Additional Plots</title>
   <link href="http://localhost:4000//rlem2021/plots"/>
   <updated>2021-10-18T00:00:00+02:00</updated>
   <id>http://localhost:4000/rlem2021/rlem2021-plots</id>
   <content type="html">&lt;p&gt;We add supplementary plots that are tested in the paper to provide additional context. In the paper, we only reported plots of the results of SLAC and PPO-LSTM, as they performed best (and only for the first year).&lt;/p&gt;

&lt;p&gt;We report the first five years of training to test the data efficiency of the algorithms, as well as the results of the test location (after 20 years of training).&lt;/p&gt;

&lt;p&gt;The parameters of the experiment are slightly different than the ones of our &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0306261921005961?via%3Dihub&quot;&gt;previous paper&lt;/a&gt;, explaining the different results (for example, SAC in this paper has a better data efficiency). We used the same weather files than in the above paper. Note that we never reuse the same weather files again to ensure that no information about the future is leaked.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;For the energy consumption plot, we used a moving average over 4 days to keep the plots legible. There are large variations in day-night cycles and it would be difficult to compare the different algorithms otherwise.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rlem2021consumption.png&quot; alt=&quot;cons&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the temperature control plots, we take a moving average over 3 hours.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rlem2021ppo.png&quot; alt=&quot;ppo&quot; class=&quot;img-responsive&quot; /&gt;
  &lt;img src=&quot;/assets/images/rlem2021ppo-lstm.png&quot; alt=&quot;ppo-lstm&quot; class=&quot;img-responsive&quot; /&gt;
  &lt;img src=&quot;/assets/images/rlem2021sac-mlp.png&quot; alt=&quot;sac&quot; class=&quot;img-responsive&quot; /&gt;
  &lt;img src=&quot;/assets/images/rlem2021sac-great.png&quot; alt=&quot;sac-lstm&quot; class=&quot;img-responsive&quot; /&gt;
  &lt;img src=&quot;/assets/images/rlem2021slac-top.png&quot; alt=&quot;slac&quot; class=&quot;img-responsive&quot; /&gt;
  &lt;img src=&quot;/assets/images/rlem2021baseline.png&quot; alt=&quot;baseline&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Implementation details and architectures</title>
   <link href="http://localhost:4000//rlem2021/hyper"/>
   <updated>2021-10-12T00:00:00+02:00</updated>
   <id>http://localhost:4000/rlem2021/rlem2021-hyper</id>
   <content type="html">&lt;p&gt;Due to the importance of hyperparameters in RL experiments, we aim to provide more details here. Due to the limited page requirements, it was not realistically possible to include it in the original paper. As a disclamer, the architectures between the algorithms differ largely in size, therefore this paper is not meant to be a benchmark study between the algorithms, but more to show that using methods for POMDPs can be valuable for some applications in energy management.&lt;/p&gt;

&lt;p&gt;For all experiments, we used a discount factor of \(\gamma=0.99\). We did not fix a seed in our experiments (something to change in future work). In the paper, we only present the results of one run, but we observed while tuning hyperparameters that each algorithm obtains consistently a similar performence (with the possible exception of SAC-LSTM, which required more tuning to obtain the results of the paper).&lt;/p&gt;

&lt;h2 id=&quot;proximal-policy-optimisation-ppo&quot;&gt;Proximal Policy Optimisation (PPO)&lt;/h2&gt;

&lt;p&gt;Recall that &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;PPO&lt;/a&gt; is an actor-critic algorithm. We are interested in finding the policy \(\pi_{\theta}(a\mid o)\) (or more generally \(\pi_{\theta}(a\mid h)\)). To do this, it is however necessary to predict the rewards in order to update the policy. Therefore, we  also need to train a critic \(V_w(s)\).&lt;/p&gt;

&lt;p&gt;The critic learns the value function \(V_w(s)\) that minimises the multistep Bellman error:&lt;/p&gt;

\[\frac{1}{2}\mathbb{E}_{\tau \sim p_{\pi}}[(V_w(s_t)-y(s_t,a_t))^2\]

&lt;p&gt;with&lt;/p&gt;

\[y\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1} \gamma^{l} r\left(s_{t+l}, a_{t+l}\right)+\gamma^{T-t} V_{w_{\mathrm{old}}}\left(s_{T}\right).\]

&lt;p&gt;This allows to compute estimates of the advantage function using Generalised Advantage Estimation (&lt;a href=&quot;https://arxiv.org/abs/1506.02438&quot;&gt;GAE&lt;/a&gt;):&lt;/p&gt;

\[\hat{A}\left(s_{t}, a_{t}\right)=\sum_{l=0}^{T-t-1}(\lambda \gamma)^{l}\left(r(s_{t+l},a_{t+l})+\gamma V_w(s_{t+l+1})-V_w(s_{t+l})\right).\]

&lt;p&gt;These estimates are used to update the policy. The policy minimises the following loss:&lt;/p&gt;

\[L_{\pi}(\theta)=\mathbb{E}_{s \sim \rho^{\pi_{\theta_{\text {old }}}}, a \sim \pi_{\theta_{\text {old }}}}\left[\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)} \hat{A}(s, a), g(\varepsilon, \hat{A}(s, a))\right)\right],\]

&lt;p&gt;where&lt;/p&gt;

\[g(\varepsilon, A)=\left\{\begin{array}{ll}
(1+\varepsilon) A &amp;amp; \text { if } A \geq 0, \\
(1-\varepsilon) A &amp;amp; \text { if } A&amp;lt;0.
\end{array}\right.\]

&lt;h3 id=&quot;ppo-with-feed-forward-networks&quot;&gt;PPO with feed-forward networks&lt;/h3&gt;

&lt;p&gt;For PPO with a feed-forward network, we used the implementation from &lt;a href=&quot;https://github.com/DLR-RM/stable-baselines3&quot;&gt;StableBaselines3&lt;/a&gt;. We kept the same hyperparameters and used the same parameters than for our &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0306261921005961&quot;&gt;prior work&lt;/a&gt;. We report the parameters in the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Critic network&lt;/td&gt;
      &lt;td&gt;\(5\rightarrow 64 \rightarrow 64 \rightarrow 1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Actor network&lt;/td&gt;
      &lt;td&gt;\(5 \rightarrow 64 \rightarrow 64 \rightarrow (2\times 4)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Activation function&lt;/td&gt;
      &lt;td&gt;Tanh&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Optimiser&lt;/td&gt;
      &lt;td&gt;Adam&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning Rate&lt;/td&gt;
      &lt;td&gt;\(3 \cdot 10^{-4}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Batch Size&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Trace-decay parameter (\(\lambda\))&lt;/td&gt;
      &lt;td&gt;0.95&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Horizon (T)&lt;/td&gt;
      &lt;td&gt;2048&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of Epochs&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Clipping Range (\(\varepsilon\))&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Global Gradient Clipping&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Entropy regularisation&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Target KL early stopping&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For more information about what these parameters do, we refer to the &lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html&quot;&gt;documentation&lt;/a&gt;. We believe that the results of PPO on this case study can be significantly improved if we take into account the recent &lt;a href=&quot;https://arxiv.org/abs/2006.05990&quot;&gt;large-scale hyperparameter studies&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;ppo-with-lstm&quot;&gt;PPO with LSTM&lt;/h3&gt;

&lt;p&gt;For an architecture with LSTM, as discussed in the paper, it is more complicated and the two networks are merged into one: 
  &lt;img src=&quot;/assets/images/lstmppo-1.png&quot; alt=&quot;lstm&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As StableBaselines3 does not contain yet an implementation of recurrent policies, we used the implementation of &lt;a href=&quot;https://github.com/hill-a/stable-baselines&quot;&gt;StableBaselines&lt;/a&gt; instead (in Tensorflow instead of Pytorch). We report the parameters in the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Neurons of Feed-forward layers (before LSTM cell)&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Neurons of LSTM layers&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Optimiser&lt;/td&gt;
      &lt;td&gt;Adam&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning Rate&lt;/td&gt;
      &lt;td&gt;\(3 \cdot 10^{-4}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Trace-decay parameter (\(\lambda\))&lt;/td&gt;
      &lt;td&gt;0.95&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Horizon (T)&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of Epochs&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Clipping range (\(\varepsilon\))&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Global Gradient Clipping&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer normalisation&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Entropy regularisation&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Target KL early stopping&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We use a lower horizon to avoid backprogating too far through time. We also use less epochs for updating the networks before collecting new data for better computational efficiency. Note that the choice of architecture can be customised in several different ways. We can also use feed-forward layers after the LSTM cell, use GRU instead of LSTM etc. Other libraries, such as &lt;a href=&quot;https://github.com/ray-project/ray&quot;&gt;RLLib&lt;/a&gt;, use different architectures.&lt;/p&gt;

&lt;h2 id=&quot;soft-actor-critic-sac&quot;&gt;Soft Actor Critic (SAC)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.05905&quot;&gt;SAC&lt;/a&gt; has shown tremendous success in recent years and became the most popular off-policy RL algorithm for continuous control. The major difference compared to other algorithms comes from the fact that it optimises the maximum entropy RL objective (with \(\alpha &amp;gt; 0\)) :&lt;/p&gt;

\[J^{\text{soft}}({\pi}) =\mathbb{E}_{\tau \sim p_{\pi}}\left[\sum_{t\geq 0}\gamma^t \left(r(s_t, a_t)-\alpha\log\pi(a_t\mid s_t)\right)\right].\]

&lt;p&gt;Similarily to many off-policy methods, the critic \(Q_w(s,a)\) is trained to minimise the Bellman error:&lt;/p&gt;

\[L(w)=\frac{1}{2}\mathbb{E}_{(s,a,s')\sim \mathcal{U}(D)}\left[\left(y(s,a,s')- Q_w(s,a)\right)^2\right],\]

&lt;p&gt;where&lt;/p&gt;

\[y\left(s, a, s^{\prime}\right)=r(s, a)+\gamma\left(\min _{j=1,2} Q_{\bar{w}_{j}}\left(s^{\prime}, a^{\prime}\right)-\alpha \log \pi_{\theta}\left(a^{\prime} \mid s^{\prime}\right)\right),\]

&lt;p&gt;where \(a'\sim \pi_{\theta}(\cdot\mid s').\) We use two target networks to reduce the overestimation bias (as in double Q-learning or TD3).&lt;/p&gt;

&lt;p&gt;The policy optimises the following objective in order to satisfy a policy improvement step:&lt;/p&gt;

\[L_{\pi}(\theta)=\mathbb{E}_{s \sim \mathcal{U}(D), a \sim \pi_{\theta}}\left[\alpha \log \pi_{\theta}(a \mid s)-Q_{w}(s, a)\right].\]

&lt;p&gt;The gradient of this loss strongly resembles the DDPG update (but with additional temperature parameter). It is common to tune the temperature as well, as the performance is sensitive to this hyperparameter and we may want to decrease its value over time (similar to the exploration parameter \(\varepsilon\) when using \(\varepsilon\)-greedy exploration for Q-learning).&lt;/p&gt;

&lt;h3 id=&quot;sac-with-feed-forward-networks&quot;&gt;SAC with feed-forward networks&lt;/h3&gt;

&lt;p&gt;The following architecture is standard. We used the implementation of &lt;a href=&quot;https://github.com/DLR-RM/stable-baselines3&quot;&gt;StableBaselines3&lt;/a&gt;. The output of the actor is \((\mu_{\theta}(s), \sigma_{\theta}(s))\); the action is then sampled from a diagonal multivariate Gaussian distribtion.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of critics&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Critic networks&lt;/td&gt;
      &lt;td&gt;\(9\rightarrow 256 \rightarrow 256\rightarrow 1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Actor network&lt;/td&gt;
      &lt;td&gt;\(5 \rightarrow 256\rightarrow 256 \rightarrow (2\times 4)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Activation function&lt;/td&gt;
      &lt;td&gt;Relu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Optimiser&lt;/td&gt;
      &lt;td&gt;Adam&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning rate&lt;/td&gt;
      &lt;td&gt;\(3 \cdot 10^{-4}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Batch size&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Polyak averaging (\(\tau\))&lt;/td&gt;
      &lt;td&gt;\(5 \cdot 10^{-3}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Temperature (\(\alpha\))&lt;/td&gt;
      &lt;td&gt;Automatically adjusted&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;sac-with-lstm&quot;&gt;SAC with LSTM&lt;/h3&gt;

&lt;p&gt;SAC with LSTM is not really standard in the literature. We found only little implementations, among them &lt;a href=&quot;https://github.com/quantumiracle/Popular-RL-Algorithms&quot;&gt;PopularRL algorithms&lt;/a&gt;, &lt;a href=&quot;https://github.com/google-research/seed_rl&quot;&gt;SeedRL&lt;/a&gt; and the implementation by &lt;a href=&quot;https://github.com/oist-cnru/Variational-Recurrent-Models&quot;&gt;Han et. al&lt;/a&gt;. We used the first one, as it was a clean implementation. All implementations use the architecture of &lt;a href=&quot;https://arxiv.org/abs/1710.06537&quot;&gt;Peng et. al&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Number of critics&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Neurons of feed-forward layers of critics&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Neurons of LSTM cell of critics&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Neurons of feed-forward layers of actor&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Neurons of LSTM cell of actor&lt;/td&gt;
      &lt;td&gt;256&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sequence length&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Batch size&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Activation function (outside of LSTM)&lt;/td&gt;
      &lt;td&gt;Relu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Optimiser&lt;/td&gt;
      &lt;td&gt;Adam&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning rate&lt;/td&gt;
      &lt;td&gt;\(3 \cdot 10^{-4}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Polyak averaging (\(\tau\))&lt;/td&gt;
      &lt;td&gt;\(10^{-2}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Temperature (\(\alpha\))&lt;/td&gt;
      &lt;td&gt;Automatically adjusted&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The policy network contains two branches:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;one recurrent branch has \((a_{t-1}, o_t)\) as input, goes through one linear layer, before entering an LSTM cell.&lt;/li&gt;
  &lt;li&gt;the second feed-forward branch has \(o_t\) as input and goes through one linear layer.&lt;/li&gt;
  &lt;li&gt;the output of both branches are concatenated and go through two other linear layers and has \((\mu_{\theta}(s), \sigma_{\theta}(s))\) as output.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The critics also contain two branches:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;one recurrent branch has \((a_{t-1}, o_t)\) as input, goes through one linear layer, before entering an LSTM cell.&lt;/li&gt;
  &lt;li&gt;the second feed-forward branch has \((a_t, o_t)\) as input and goes through one linear layer.&lt;/li&gt;
  &lt;li&gt;the output of both branches are concatenated and go through two other linear layers and has its Q-value as output.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The motivation of using two branches is that the recurrent branch is supposed to infer the dynamics of the environment, making the approach arguably model-based. As the current observation \(o_t\) is of fundamental importance, it is preprocessed by another branch to make sure that this information is still available after going through the LSTM cell.&lt;/p&gt;

&lt;h2 id=&quot;stochastic-latent-actor-critic-slac&quot;&gt;Stochastic Latent Actor Critic (SLAC)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.00953&quot;&gt;SLAC&lt;/a&gt; is a generalisation of SAC to POMDPs. The derivation of the objective follows the same idea as SAC. It uses the &lt;a href=&quot;https://arxiv.org/abs/1805.00909&quot;&gt;control as inference&lt;/a&gt; framework, which interprets control as a probabilistic inference problem. This can be used to derive the maximum entropy RL objective that is used by SAC. If we apply the same ideas to a POMDP, instead of an MDP, we arrive at the objective of SLAC.&lt;/p&gt;

&lt;p&gt;The actor and critic losses are therefore identical to SAC; the only difference comes from the fact that we also need to approximate the belief. We have, therefore, this loss as well:&lt;/p&gt;

\[\begin{align}
J_M(\varphi, \psi) &amp;amp;= \mathbb{E}_{s_{1:\tau + 1}\sim q_{\varphi}}\Big[\sum_{t=0}^{\tau}\Big(-\log e_{\psi}(o_{t+1}\mid s_{t+1}) \\ &amp;amp;+ \mathcal{D}_{KL}(q_{\varphi}(s_{t+1}\mid o_{t+1}, s_t, a_t)\Vert p_{\psi}(s_{t+1}\mid s_t, a_t))\Big)\Big]. 
\end{align}\]

&lt;p&gt;We separated the contribution to the encoder weights \(\varphi\) and the decoder weights \(\psi\). The intuiton behind this is that the networks have different roles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The encoder \(q_{\varphi}\) aims to approximate the belief, that is it aims to learn a latent representation of the state. This state can then used by the critic to estimate future rewards.&lt;/li&gt;
  &lt;li&gt;The observation decoder \(e_{\psi}\) aims to generate samples that resemble the ones given by the environment.&lt;/li&gt;
  &lt;li&gt;The transition decoder \(p_{\psi}\) aims to model the environment, so that the state is updated when an action is taken in a similar way than the “true” state of the environment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore the loss means that on one hand we want to maximise the likelihood of observations \((\hat{o}_1,\cdots, \hat{o}_{\tau + 1})\) genrated by the decoder. And on the other hand, we want to minimise the Kullback-Leibler divergence between the sequence of states \((s_1, \cdots, s_{\tau + 1})\) inferred by the encoder and the sequence of states \((\hat{s}_1, \cdots, \hat{s}_{\tau + 1})\) generated by the decoder.&lt;/p&gt;

&lt;p&gt;Traditional VAE do not evaluate sequences. SLAC uses a Bayesian network to address this issue, but related work also use recurrent VAE. A small (of length 8 in the paper) sequence of observations and actions are collected in the environment (using an older version of the policy). These sequences are then stored into a replay buffer. The networks are then updated the following way:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample a batch of observation-action sequences \((o_1, a_1, \cdots, o_{\tau + 1}, a_{\tau + 1})\) from the replay buffer.&lt;/li&gt;
  &lt;li&gt;The first observation \(o_1\) goes through the encoder \(q_{\varphi}(s_1\mid o_1)\). \(q_{\varphi}\) is a two-layer feed-forward network, that uses \(o_1\) as input.&lt;/li&gt;
  &lt;li&gt;The latent state \(s_1\) is used as input by the critic \(Q_w(s_1, a_1)\).&lt;/li&gt;
  &lt;li&gt;The latent state is used as input by \(e_{\psi}(o_1\mid s_1)\) to generate an observation \(\hat{o}_1\).&lt;/li&gt;
  &lt;li&gt;The latent state is also used as input by \(p_{\psi}(s_2\mid s_1, a_1)\) to generate a next state \(\hat{s}_2\). This is a network that takes \((s_1, a_1)\) as input.&lt;/li&gt;
  &lt;li&gt;The latent state \(s_1\) is then also used to calculate the next latent state \(s_2\). Here \(q_{\varphi}(s_2\mid s_1, a_1, o_2)\) is a neural network that takes \((s_1,a_1,o_2)\) as input and is different to the network, used in step 1.&lt;/li&gt;
  &lt;li&gt;We continue this way to generate the necessary sequences.&lt;/li&gt;
  &lt;li&gt;We can then calculate the desired losses and update the networks using gradient descent (or Adam in practice).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This implies that we have 7 neural networks to train: 2 critics, 1 actor, 2 decoders and 2 encoders (one for the first time-step using only the initial observation as input, the other using the current observation, along with the previous state and action as input). However, as we shall see, the original implementation is more complicated…&lt;/p&gt;

&lt;h3 id=&quot;the-actual-implementation-with-latent-variable-factorisation&quot;&gt;The actual implementation with latent variable factorisation…&lt;/h3&gt;

&lt;p&gt;Lee et. al use two-variable factorisation model, meaning that they write \(s_t=(z_t^1, z_t^2)\) and write&lt;/p&gt;

\[q_{\varphi}(s_1\mid o_1) = q_{\varphi}(z^2_1\mid z_1^1)q_{\varphi}(z_1^1\mid o_1)\]

&lt;p&gt;and&lt;/p&gt;

\[q_{\varphi}(s_t\mid o_t, a_{t-1}, s_{t-1})=\textcolor{blue}{q_{\varphi}(z^2_t\mid z^1_t,z^2_{t-1},a_{t-1})}q_{\varphi}(z^1_t\mid o_t, z^2_{t-1},a_{t-1}).\]

&lt;p&gt;We do a similar factorisation for the decoder:&lt;/p&gt;

\[p_{\psi}(s_t\mid s_{t-1}, a_{t-1})=\textcolor{blue}{p_{\psi}(z_t^2\mid z^1_t,z^2_{t-1},a_{t-1})}p_{\psi}(z^1_t\mid z^2_{t-1},a_{t-1}).\]

&lt;p&gt;The terms in blue have the same input, so a common simplification is to use the same network \(p_{\psi}(z_t^2\mid z^1_t,z^2_{t-1},a_{t-1})\) for both the encoder and decoder. This design simplifies the training process. This idea to combine VAE with autoregressive models is for instance discussed by &lt;a href=&quot;https://arxiv.org/abs/1902.02102&quot;&gt;Maaløe et. al.&lt;/a&gt; and has been used in image generation tasks, obtaining competitive performance to generative adversarial networks (GAN). Lee et. al. performed an ablation study and showed that this design significantly improves performance.&lt;/p&gt;

&lt;p&gt;Another important design choice is whether the policy \(\pi_{\theta}\) should use the latent state \(s_t\sim q_{\varphi}\) as input or be conditioned on the history \(h_t=(o_1,a_1,\cdots, o_t)\). There is some debate on what choice should be used. SLAC uses the history as input and argues that using the latent state as input leads to over-optimistic behaviour. Furthermore, during runtime, we can run the policy without requiring an inference model. However, &lt;a href=&quot;https://arxiv.org/abs/1912.10703&quot;&gt;Han et. al.&lt;/a&gt; argues that this choice makes it impossible for the agent to remember long-term dependencies, as the policy only takes into account the recent past. An ablation study is done in the SLAC paper and similar results are obtained. In our experiments, we used the history as imput. However, the critic \(Q_w(s_t,a_t)\) is conditioned on the state \(s_t\sim q_{\varphi}\) and obtains significantly worse results otherwise.&lt;/p&gt;

&lt;p&gt;Another network is used to model the reward function. However, we believe that for our application, this design choice is not essential, as we defined the reward function with the current observation only, so it is not difficult to infer. However, if the reward function is random or non-Markovian, this additional network may be beneficial. Therefore, using the notations of the two-variable factorisation, we have in addition the reward decoder:&lt;/p&gt;

\[\hat{r}_{t+1} \sim r_{\psi}(\cdot\mid z^1_t,z_t^2,a_t,z_{t+1}^1,z_{t+1}^2).\]

&lt;p&gt;Therefore, a sequence of rewards \((\hat{r}_1, \cdots, \hat{r}_{\tau})\) is also generated and another likelihood term for the rewards should be added to the loss function. This means that the state should not only include information to explain observations, but also the rewards.&lt;/p&gt;

&lt;p&gt;Summing up, the actual loss function for the VAE is the following:&lt;/p&gt;

\[\begin{align} 
J_M(\varphi, \psi) &amp;amp;= \mathbb{E}_{z^1_{1:\tau + 1}\sim q_{\varphi}, z^2_{1:\tau +1}\sim p_{\psi}}\Big[\sum_{t=0}^{\tau}\Big(-\log e_{\psi}(o_{t+1}\mid z^1_{t+1}, z_{t+1}^2) \\ &amp;amp;-\log r_{\psi}(r_{t+1}\mid z^1_{t},z_{t}^2,a_{t},z_{t+1}^1,z_{t+1}^2) \\ &amp;amp;+ \mathcal{D}_{KL}(q_{\varphi}(z^1_{t+1}\mid o_{t+1}, z^2_t, a_t)\Vert p_{\psi}(z^1_{t+1}\mid z^2_t, a_t))\Big)\Big].
\end{align}\]

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;An important additional change is necessary to adapt the architecture to the current case study. The SLAC paper has been implemented for image observations, so we need to adapt it to vector observations. This for example done by &lt;a href=&quot;https://arxiv.org/abs/1912.10703&quot;&gt;Han et. al.&lt;/a&gt; and we used the implementation available &lt;a href=&quot;https://github.com/oist-cnru/Variational-Recurrent-Models&quot;&gt;here&lt;/a&gt;. This implementation does the following design choices:&lt;/p&gt;

&lt;p&gt;It uses the original version of SAC, which trains two critics: one critic \(Q_w(s,a)\) and the critic \(V_{\omega}(s)\). The Bellman targets use \(V_{\bar{\omega}}(s)\), hence \(V_{\omega}(s)\) is a network aiming to stabilise the Bellman residuals. The more commonly used version of SAC uses a target Q-network, as it involves training a network less. The implementation however uses temperature tuning, as introduced by SACv2.&lt;/p&gt;

&lt;p&gt;They use two different networks to model \(\mu\) and \(\sigma\) respectively, each consisting of two layers without shared weights. This is different to the commonly used architectures in RL (e.g. in StableBaselines3), where the input goes through two linear layers and then goes to two separate heads (one for \(\mu\) and one for \(\sigma\)). In this paper, we used (because we already performed the experiments, before going through the code) the same design than Han et. al., but we believe that a modification here can significantly reduce computational time, without hurting performance.&lt;/p&gt;

&lt;p&gt;All neural networks are feed-forward networks and both \(\mu\) and \(\sigma\) are trainable parameters. In the original paper, they used a fixed  \(\sigma\) for \(e_{\psi}\) to generate images. Furthermore, the actor is conditioned on past observations \((o_1,\cdots,o_{t})\) instead of the history \((o_1,a_1,\cdots, a_{t-1},o_t).\) The input of the policy is the dimension of observation space * sequence length. To deal with missing data, zero padding is used.&lt;/p&gt;

&lt;p&gt;All the networks trained are summarised in the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sequence length&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dimension of \(z^1\)&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Dimension of \(z^2\)&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Actor network \(\pi_{\theta}(a_t\mid o_1,\cdots,o_t)\)&lt;/td&gt;
      &lt;td&gt;\(2\times(5\cdot 8\rightarrow 256 \rightarrow 256 \rightarrow 4)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Value network \(V_{\omega}(z^2_t)\)&lt;/td&gt;
      &lt;td&gt;\(32 \rightarrow 256 \rightarrow 256 \rightarrow 1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Q-networks \(Q_{w_i}(z^2_t,a_t)\)&lt;/td&gt;
      &lt;td&gt;\(2\times ((32+4)\rightarrow 256 \rightarrow 256 \rightarrow 1)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Transition decoder \(p_{\psi}(z^1_{t+1}\mid z^2_t, a_t)\)&lt;/td&gt;
      &lt;td&gt;\(2\times ((32 + 4)\rightarrow 128 \rightarrow 128 \rightarrow 16)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Common encoder/decoder \(p_{\psi}(z^2_{t+1}\mid z^1_{t+1},z^2_t,a_t)\)&lt;/td&gt;
      &lt;td&gt;\(2\times((32 + 16 + 4)\rightarrow 128 \rightarrow 128 \rightarrow 32)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Initial transition decoder \(p_{\psi}(z^2_1\mid z^1_1)\)&lt;/td&gt;
      &lt;td&gt;\(2\times(16 \rightarrow 128 \rightarrow 128\rightarrow 32)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Emission decoder \(e_{\psi}(o_t\mid z^1_t,z^2_t)\)&lt;/td&gt;
      &lt;td&gt;\(2\times ((32+16)\rightarrow 128\rightarrow 128\rightarrow 5)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Reward decoder \(r_{\psi}(r_{t+1}\mid z^1_t,z_t^2,a_t,z_{t+1}^1,z_{t+1}^2)\)&lt;/td&gt;
      &lt;td&gt;\(2\times((2\cdot(32+16)+4)\rightarrow 128\rightarrow 128\rightarrow 1)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Belief encoder \(q_{\varphi}(z^1_{t+1}\mid o_{t+1},z^2_t,a_t)\)&lt;/td&gt;
      &lt;td&gt;\(2\times((32+5+4)\rightarrow 128 \rightarrow 128 \rightarrow 16)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Initial belief encoder \(q_{\varphi}(z^1_1\mid o_1)\)&lt;/td&gt;
      &lt;td&gt;\(2\times (5\rightarrow 128 \rightarrow 128\rightarrow 16)\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Batch size&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Activation functions of VAE&lt;/td&gt;
      &lt;td&gt;Tanh&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Activation function of actor/critics&lt;/td&gt;
      &lt;td&gt;Relu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Optimiser&lt;/td&gt;
      &lt;td&gt;Adam&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning rate of VAE&lt;/td&gt;
      &lt;td&gt;\(10^{-4}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning rate of actor/critcs&lt;/td&gt;
      &lt;td&gt;\(3\cdot 10^{-4}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Polyak averaging&lt;/td&gt;
      &lt;td&gt;\(5\cdot 10^{-3}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Temperature (\(\alpha\))&lt;/td&gt;
      &lt;td&gt;Automatically adjusted&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Model pretraining&lt;/td&gt;
      &lt;td&gt;5000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data collection before training starts&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The last two lines mean that we collect data during 1000 timesteps (around 10 days) before pretraining the model. This explains the large performance improvement of SLAC (and SAC-LSTM using a similar trick) at the beginning of the training. This trick can significantly improve data efficiency and may be of interest for other algorithms.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;The different architectures vary in size by several order of magnitudes, so this is not meant to be a fair comparison between the algorithms. Obviously, SAC-LSTM and SLAC with the current architecture sizes require a GPU. The RAM requirements of SLAC are also high. We found that smaller networks in SLAC (e.g. with around 64 neurons) do not reduce the performence improvements by much, while still enjoying impressive data efficiency. This makes it technically possible to train a model on a CPU, as the additonal gains in data efficiency compensate for the higher computational time.&lt;/p&gt;

</content>
 </entry>
 

</feed>
